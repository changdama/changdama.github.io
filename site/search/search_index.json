{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Changda Ma","text":""},{"location":"README%20%281%29/","title":"COSMIC-PULSES","text":"<p>\"Sound Architecture Machine\"-Explore the Interactive Relationship between Sound, Human Activities and Architecture</p>"},{"location":"README%20%281%29/#introduction","title":"INTRODUCTION","text":"<p>Architecture space always serves human beings, and the main task of architects is to design spaces that are suitable for human beings. But does the so-called \u201chuman-friendly\u201d space designed by architects really meet people\u2019s needs? Therefore, the adaptability of space should be determined by people\u2019s activities in the space.</p> <p>Why human beings need to move in architecture space, they need a purpose to pull them to generate motivation, so human beings and architecture need a motivation or a factor to coordinate. Music is a great factor. 1970s, the birth of avant-garde music, avant-garde music composers usually use symbolic notation to record the notes, melody, and rhythm, etc., the music creates new graphic combinations of characteristics, and graphics also form a new music, the music is also a new graphic combination of characteristics. The music created new graphic combinations and the graphics created new musical forms, an interaction that can be translated to music, architecture and human activity. Nature is boundless, it symbolizes freedom, similar to the freedom of movement in architecture space, so eco-accoustic is a kind of musical freedom of movement.</p>"},{"location":"README%20%281%29/#sound-selected","title":"SOUND SELECTED","text":""},{"location":"README%20%281%29/#stackhausens-piece-cosmic-pulses","title":"Stackhausen's piece \"COSMIC PULSES\"","text":"<p>Cosmic Pulses is the 13th \"hour\" of Stockhausen's originally-planned 24-part cycle KLANG (\"SOUND\") which is based on the 24 hours of the day. Thiselectronic work is composed of 24 layers of synthesizer-generated melodic material, with each layer having a different speed and pitch register. The layers enter one by one, starting from the lowest/slowest layer, and go up in sequence to the highest/fastest layer.</p>"},{"location":"README%20%281%29/#ecoacoustics","title":"Ecoacoustics","text":"<p>Ecoacoustics is an interdisciplinary science that investigates natural and anthropogenic sounds and their relationship with the environment over a wide range of study scales, both spatial and temporal, including populations, communities, and landscapes. Ecoacoustics operates in all types of terrestrial and aquatic (freshwater and marine) ecosystems.</p> <p></p>"},{"location":"README%20%281%29/#sound-machine-principle-1","title":"SOUND MACHINE PRINCIPLE 1","text":""},{"location":"README%20%281%29/#description","title":"Description","text":"<p>Players: 24 Different Sounds, 8 from each of 3 categories- Anthro, Bio &amp;Geophony</p> <p>Gameboard: Wheel with 3 Concentric Rings of Different Sounds</p> <p>Rule:  - Play of 24 sounds in different combinations - 8 sounds in 3 layers playing in random orders but fixed time intervals - Time Interval between each sounds is according to the Fibonacci Series</p> <p>Result: The result is a creation of a unique sound machine in each combination</p> <p></p>"},{"location":"README%20%281%29/#sound-machine-operation-system-for-one-track","title":"Sound Machine Operation System for One Track","text":"<p>\ud83c\udfa5 Watch the Video </p>"},{"location":"README%20%281%29/#sound-machine-operation-system-for-three-tracks","title":"Sound Machine Operation System for Three Tracks","text":"<p>\ud83c\udfa5 Watch the Video </p> <ul> <li>Improvisation</li> </ul> <p>Another max patch is added to randomly change the thresholds of the low shelf gain, mid gain, and high shelf gain of each group of voices to enrich the \"sound machine\" effect by improvising the control of the three low shelf gain, mid gain, and high shelf    buttons on the spot to randomize the thresholds of the voices.</p> <p></p> <ul> <li> <p>Sound-optimized design</p> <p>Each category has a bass part, an arpeggiator, and a synth playing backup chords\u3002Each sound machine contains one of them. The selection of instruments will be as complementary as possible to the samples. For example, bird samples with higher pitches and denser     sounds will be paired with bass parts with lower pitches and longer notes. In contrast, the samples of summer night insects are paired with an arpeggiator, which can light up the whole section. All the notes or chords only change when the sound machine switches to a    new sample.</p> </li> </ul> <p></p> <pre><code> Besides, instrumental parts from each category use different scales. However, to ensure natural transition when approaching sound machines from a different category, and a harmonious overall effect, the scales are closely related to each other: g natural minor\n</code></pre> <p>for Biophony, F major for Anthrophony, bB major for Geophony. This consideration mainly roots from our vision of harmonious coexistence between human and nature.</p>"},{"location":"README%20%281%29/#sound-machine-principle-2","title":"SOUND MACHINE PRINCIPLE 2","text":""},{"location":"README%20%281%29/#description_1","title":"Description","text":"<p>Players: 8 different sound machines.</p> <p>Gameboard: Box with a medium divided into 24 layers.</p> <p>Rules:  - Play of 8 different sound machines in different combinations &amp; positions. - Movement in positive &amp; negative X direction. - Movement in positive &amp; negative Y direction.</p> <p>Result: The result is a creation of a interactive soundscape relative to position.</p>"},{"location":"README%20%281%29/#effect-of-one-sound-machine-in-medium","title":"Effect of One Sound Machine in Medium","text":"<p>For the effects in the medium, The sound generates spatial lines in the medium of unity, and as the sound travels through the medium, the sparsity of the spatial lines changes\uff0cwhich like sound creates ripples in the water.</p>"},{"location":"README%20%281%29/#soundscape-sound-machines-in-medium","title":"SOUNDSCAPE: SOUND MACHINES IN MEDIUM","text":""},{"location":"README%20%281%29/#section-of-sound-machines-in-soundscape","title":"Section of Sound Machines in Soundscape","text":"<p>Sound Waves of Different Sound Machines: </p> <p></p> <p>Sound Machines Movement:</p> <p>Dissecting the medium into 24 layers and exploring the iterations of different movements and across different time periods.</p> <p></p> <p>The wave forms of Sound Machines are analysed at different time periods to visualise the frequency and displacement. They are then subjected to a box with a medium and targeted with a point source of sound in different positions. The final diagrams are the visualisations of these sound machines interacting with the medium and each other.(Take \"SAME TIME PERIOD POSITIONS: TOP 4 MACHINES IN \u2013X DIRECTION, BOTTOM 4 MACHINES IN +X DIRECTION\" as an example)</p> <p></p>"},{"location":"README%20%281%29/#sound-machines-people-and-space-interaction","title":"SOUND MACHINES, PEOPLE AND SPACE INTERACTION","text":""},{"location":"README%20%281%29/#description-of-each-interactive-factors","title":"Description of each interactive factors","text":"<p>The interactive envionmenet is built in Unity, envisioning it as a medium. Sound, people and medium are interacted with each other. Based on two principles, We put eight \"sound machines\"\uff0cthey make rotating sounds through their respective black sound speakers.</p> <p></p>"},{"location":"README%20%281%29/#interaction-with-person","title":"Interaction With Person","text":"<p>For interaction with people, as the person gets closer to the speaker, the sound gets progressively louder and the spatial lines produced in the medium get denser; the further the person gets away from the speaker, the sound gets progressively smaller and the spatial lines produced in the medium get sparse.</p> <p>\ud83c\udfa5 Watch the Video</p> <p> </p>"},{"location":"README%20%281%29/#overall-interaction","title":"Overall Interaction","text":"<ul> <li>Video </li> </ul> <p>\ud83c\udfa5 Watch the Video</p> <p>\ud83c\udfac Click here to download the video(Best Quality)</p> <p> </p> <ul> <li>Unity Interaction</li> <li> <p>Project Structure</p> <ul> <li><code>.vscode</code>:Configuration files for the Visual Studio Code editor.</li> <li><code>Assets</code>:The core folder of the project, containing all resources (models, scripts, materials)</li> <li><code>Packages</code>: Configuration files for the Unity Package Manager.</li> <li><code>ProjectSettings</code>: Configuration files for project settings.</li> </ul> </li> <li> <p>Requirements</p> <p> </p> </li> <li> <p>Start</p> <ul> <li>Using Git:</li> <li><code>git clone https://github.com/changdama/COSMIC-PULSES.git</code></li> <li>Download ZIP</li> <li>Open Unity Hub, Click <code>Add</code>and navigate to the folder containing this project, and Select the project and click <code>Open</code></li> </ul> </li> </ul>"},{"location":"cv/","title":"CV","text":""},{"location":"cv/#preview","title":"Preview","text":""},{"location":"projects/","title":"Index","text":""},{"location":"projects/#projects","title":"Projects","text":""},{"location":"projects/#fall-2025","title":"Fall 2025","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> XR Instruments     Multi-headset collaborative XR musical instruments and OSC workflows. Team: Changda Ma, ... \u2192 Learn more</p> </li> </ul>"},{"location":"projects/Audio_Mixing_and_Recordings/","title":"Overview","text":""},{"location":"projects/Audio_Mixing_and_Recordings/#audio-mixing-and-recordings","title":"Audio Mixing and Recordings:","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> COSMIC PULSES  (Course Project)</p> <p>\"Sound Architecture Machine\"-Using Unity to Explore The Interactive Relationship between Sound, Human Activities and Architecture. Team: Changda Ma, Ruthu Chandrashekar, Yurui Wu, Qufei Hou </p> <p>Instructors: Lars Spuijbroek, Frank Clark</p> <p>\u2192 Learn more</p> </li> </ul>"},{"location":"projects/architecture/","title":"Overview","text":""},{"location":"projects/architecture/#projects","title":"Projects","text":""},{"location":"projects/architecture/#high-performance-buildings","title":"High Performance Buildings:","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> XR Instruments     Multi-headset collaborative XR musical instruments and OSC workflows. Team: Changda Ma, ... \u2192 Learn more</p> </li> </ul>"},{"location":"projects/architecture/#ai-for-built-environment","title":"AI for Built Environment:","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> XR Instruments     Multi-headset collaborative XR musical instruments and OSC workflows. Team: Changda Ma, ... \u2192 Learn more</p> </li> </ul>"},{"location":"projects/architecture/#built-environment-and-health","title":"Built Environment and Health:","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> XR Instruments     Multi-headset collaborative XR musical instruments and OSC workflows. Team: Changda Ma, ... \u2192 Learn more</p> </li> </ul>"},{"location":"projects/architecture/#digital-fabrication-and-robotics","title":"Digital Fabrication and Robotics:","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> XR Instruments     Multi-headset collaborative XR musical instruments and OSC workflows. Team: Changda Ma, ... \u2192 Learn more</p> </li> </ul>"},{"location":"projects/architecture/#computational-design","title":"Computational Design:","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> XR Instruments     Multi-headset collaborative XR musical instruments and OSC workflows. Team: Changda Ma, ... \u2192 Learn more</p> </li> </ul>"},{"location":"projects/music_tech/","title":"Overview","text":""},{"location":"projects/music_tech/#music-technology","title":"Music Technology","text":""},{"location":"projects/music_tech/#interactive-sound-xr-experience-and-spatial-audio","title":"Interactive Sound, XR Experience and Spatial Audio:","text":"<ul> <li> <p>Cube-Network XR </p> <p>(Lab Ongoing Research)</p> <p>An Experimental System for Networked and hardware XR Musical Interaction  </p> <p>Team: Canting Zhu, Changda Ma, Tristan Peng, Lana Boudiab</p> <p>Instructors: Henrik von Coler</p> <p>\u2192 Learn more</p> </li> <li> <p></p> <p>Since Everyone is DJ </p> <p>(Course Project and Ongoing Research)</p> <p>Spatial Audio Interactive Installation presented at The Underground, Atlanta.  </p> <p>Team: Canting Zhu, Changda Ma, Gibran Mubarak, Sunshiyu Wang</p> <p>Instructors: Henrik von Coler</p> <p>\u2192 Learn more</p> </li> <li> <p> COSMIC PULSES  (Course Project)</p> <p>\"Sound Architecture Machine\"-Using Unity to Explore The Interactive Relationship between Sound, Human Activities and Architecture. Team: Changda Ma, Ruthu Chandrashekar, Yurui Wu, Qufei Hou </p> <p>Instructors: Lars Spuijbroek, Frank Clark</p> <p>\u2192 Learn more</p> </li> <li> <p></p> <p>Music Hide  (Course Project)</p> <p>AR-based multiplayer musical treasure hunt  </p> <p>Team: Dhruv, Changda Ma</p> <p>Instructors: Jeremy Muller</p> <p>\u2192 Learn more</p> </li> </ul>"},{"location":"projects/music_tech/#ai-for-music","title":"AI for Music:","text":"<ul> <li> <p>MusicGen-Rhythm (Ongoing research) </p> <p>Rhythm-aware conditioning for text-to-music audio generation </p> <p>Team: Changda Ma, Govinda Madhava BS, Lennon Seiders</p> <p>Instructors: Claire Arthur, Noel Alben (TA) </p> <p>\u2192 Learn more</p> </li> <li> <p></p> <p>Deep Salient Detection for F0 Estimation (Course Project)  </p> <p>Multi-headset collaborative XR musical instruments and OSC workflows.  </p> <p>Team: Changda Ma, Jiayi Wang </p> <p>Instructor: Claire Arthur, Noel Alben (TA)</p> <p>\u2192 Learn more</p> </li> </ul>"},{"location":"projects/music_tech/#music-information-retrieval","title":"Music Information Retrieval:","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> XR Instruments     Multi-headset collaborative XR musical instruments and OSC workflows. Team: Changda Ma, Alexey Voronin      \u2192 Learn more</p> </li> </ul>"},{"location":"projects/music_tech/#music-symbolic-and-perception","title":"Music Symbolic and Perception:","text":"<ul> <li> <p>Exploring the Influence of Syncopation and Loudness on Listener Arousal in Music:  (Course Project)</p> <p>Using HumdrumR to examine the individual and combined effects of syncopation and loudness on listener reported arousal. </p> <p>Team: Changda Ma, Alexey Voronin </p> <p>Instructors: Claire Arthur</p> <p>\u2192 Learn more</p> </li> </ul>"},{"location":"projects/music_tech/#sonification","title":"Sonification:","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> XR Instruments     Multi-headset collaborative XR musical instruments and OSC workflows. Team: Changda Ma, ... \u2192 Learn more</p> </li> </ul>"},{"location":"projects/music_tech/#acoustics","title":"Acoustics:","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> XR Instruments     Multi-headset collaborative XR musical instruments and OSC workflows. Team: Changda Ma, ... \u2192 Learn more</p> </li> </ul>"},{"location":"projects/music_tech/COSMIC_PULSES/","title":"COSMIC-PULSES","text":"<p>\"Sound Architecture Machine\"-Explore the Interactive Relationship between Sound, Human Activities and Architecture</p>"},{"location":"projects/music_tech/COSMIC_PULSES/#introduction","title":"INTRODUCTION","text":"<p>Architecture space always serves human beings, and the main task of architects is to design spaces that are suitable for human beings. But does the so-called \u201chuman-friendly\u201d space designed by architects really meet people\u2019s needs? Therefore, the adaptability of space should be determined by people\u2019s activities in the space.</p> <p>Why human beings need to move in architecture space, they need a purpose to pull them to generate motivation, so human beings and architecture need a motivation or a factor to coordinate. Music is a great factor. 1970s, the birth of avant-garde music, avant-garde music composers usually use symbolic notation to record the notes, melody, and rhythm, etc., the music creates new graphic combinations of characteristics, and graphics also form a new music, the music is also a new graphic combination of characteristics. The music created new graphic combinations and the graphics created new musical forms, an interaction that can be translated to music, architecture and human activity. Nature is boundless, it symbolizes freedom, similar to the freedom of movement in architecture space, so eco-accoustic is a kind of musical freedom of movement.</p>"},{"location":"projects/music_tech/COSMIC_PULSES/#sound-selected","title":"SOUND SELECTED","text":""},{"location":"projects/music_tech/COSMIC_PULSES/#stackhausens-piece-cosmic-pulses","title":"Stackhausen's piece \"COSMIC PULSES\"","text":"<p>Cosmic Pulses is the 13th \"hour\" of Stockhausen's originally-planned 24-part cycle KLANG (\"SOUND\") which is based on the 24 hours of the day. This electronic work is composed of 24 layers of synthesizer-generated melodic material, with each layer having a different speed and pitch register. The layers enter one by one, starting from the lowest/slowest layer, and go up in sequence to the highest/fastest layer.</p>"},{"location":"projects/music_tech/COSMIC_PULSES/#ecoacoustics","title":"Ecoacoustics","text":"<p>Ecoacoustics is an interdisciplinary science that investigates natural and anthropogenic sounds and their relationship with the environment over a wide range of study scales, both spatial and temporal, including populations, communities, and landscapes. Ecoacoustics operates in all types of terrestrial and aquatic (freshwater and marine) ecosystems.</p> <p></p>"},{"location":"projects/music_tech/COSMIC_PULSES/#sound-machine-principle-1","title":"SOUND MACHINE PRINCIPLE 1","text":""},{"location":"projects/music_tech/COSMIC_PULSES/#description","title":"Description","text":"<p>Players: 24 Different Sounds, 8 from each of 3 categories- Anthro, Bio &amp;Geophony</p> <p>Gameboard: Wheel with 3 Concentric Rings of Different Sounds</p> <p>Rule:  - Play of 24 sounds in different combinations - 8 sounds in 3 layers playing in random orders but fixed time intervals - Time Interval between each sounds is according to the Fibonacci Series</p> <p>Result: The result is a creation of a unique sound machine in each combination</p> <p></p>"},{"location":"projects/music_tech/COSMIC_PULSES/#sound-machine-operation-system-for-one-track","title":"Sound Machine Operation System for One Track","text":"<p>\ud83c\udfa5 Watch the Video </p>"},{"location":"projects/music_tech/COSMIC_PULSES/#sound-machine-operation-system-for-three-tracks","title":"Sound Machine Operation System for Three Tracks","text":"<p>\ud83c\udfa5 Watch the Video </p> <ul> <li>Improvisation</li> </ul> <p>Another max patch is added to randomly change the thresholds of the low shelf gain, mid gain, and high shelf gain of each group of voices to enrich the \"sound machine\" effect by improvising the control of the three low shelf gain, mid gain, and high shelf    buttons on the spot to randomize the thresholds of the voices.</p> <p></p> <ul> <li> <p>Sound-optimized design</p> <p>Each category has a bass part, an arpeggiator, and a synth playing backup chords\u3002Each sound machine contains one of them. The selection of instruments will be as complementary as possible to the samples. For example, bird samples with higher pitches and denser     sounds will be paired with bass parts with lower pitches and longer notes. In contrast, the samples of summer night insects are paired with an arpeggiator, which can light up the whole section. All the notes or chords only change when the sound machine switches to a    new sample.</p> </li> </ul> <p></p> <pre><code> Besides, instrumental parts from each category use different scales. However, to ensure natural transition when approaching sound machines from a different category, and a harmonious overall effect, the scales are closely related to each other: g natural minor\n</code></pre> <p>for Biophony, F major for Anthrophony, bB major for Geophony. This consideration mainly roots from our vision of harmonious coexistence between human and nature.</p>"},{"location":"projects/music_tech/COSMIC_PULSES/#sound-machine-principle-2","title":"SOUND MACHINE PRINCIPLE 2","text":""},{"location":"projects/music_tech/COSMIC_PULSES/#description_1","title":"Description","text":"<p>Players: 8 different sound machines.</p> <p>Gameboard: Box with a medium divided into 24 layers.</p> <p>Rules:  - Play of 8 different sound machines in different combinations &amp; positions. - Movement in positive &amp; negative X direction. - Movement in positive &amp; negative Y direction.</p> <p>Result: The result is a creation of a interactive soundscape relative to position.</p>"},{"location":"projects/music_tech/COSMIC_PULSES/#effect-of-one-sound-machine-in-medium","title":"Effect of One Sound Machine in Medium","text":"<p>For the effects in the medium, The sound generates spatial lines in the medium of unity, and as the sound travels through the medium, the sparsity of the spatial lines changes\uff0cwhich like sound creates ripples in the water.</p>"},{"location":"projects/music_tech/COSMIC_PULSES/#soundscape-sound-machines-in-medium","title":"SOUNDSCAPE: SOUND MACHINES IN MEDIUM","text":""},{"location":"projects/music_tech/COSMIC_PULSES/#section-of-sound-machines-in-soundscape","title":"Section of Sound Machines in Soundscape","text":"<p>Sound Waves of Different Sound Machines: </p> <p></p> <p>Sound Machines Movement:</p> <p>Dissecting the medium into 24 layers and exploring the iterations of different movements and across different time periods.</p> <p></p> <p>The wave forms of Sound Machines are analysed at different time periods to visualise the frequency and displacement. They are then subjected to a box with a medium and targeted with a point source of sound in different positions. The final diagrams are the visualisations of these sound machines interacting with the medium and each other.(Take \"SAME TIME PERIOD POSITIONS: TOP 4 MACHINES IN \u2013X DIRECTION, BOTTOM 4 MACHINES IN +X DIRECTION\" as an example)</p> <p></p>"},{"location":"projects/music_tech/COSMIC_PULSES/#sound-machines-people-and-space-interaction","title":"SOUND MACHINES, PEOPLE AND SPACE INTERACTION","text":""},{"location":"projects/music_tech/COSMIC_PULSES/#description-of-each-interactive-factors","title":"Description of each interactive factors","text":"<p>The interactive envionmenet is built in Unity, envisioning it as a medium. Sound, people and medium are interacted with each other. Based on two principles, We put eight \"sound machines\"\uff0cthey make rotating sounds through their respective black sound speakers.</p> <p></p>"},{"location":"projects/music_tech/COSMIC_PULSES/#interaction-with-person","title":"Interaction With Person","text":"<p>For interaction with people, as the person gets closer to the speaker, the sound gets progressively louder and the spatial lines produced in the medium get denser; the further the person gets away from the speaker, the sound gets progressively smaller and the spatial lines produced in the medium get sparse.</p> <p>\ud83c\udfa5 Watch the Video</p> <p> </p>"},{"location":"projects/music_tech/COSMIC_PULSES/#overall-interaction","title":"Overall Interaction","text":"<ul> <li>Video </li> </ul> <p>\ud83c\udfa5 Watch the Video</p> <p>\ud83c\udfac Click here to download the video(Best Quality)</p> <p> </p> <ul> <li>Unity Interaction</li> <li> <p>Project Structure</p> <ul> <li><code>.vscode</code>:Configuration files for the Visual Studio Code editor.</li> <li><code>Assets</code>:The core folder of the project, containing all resources (models, scripts, materials)</li> <li><code>Packages</code>: Configuration files for the Unity Package Manager.</li> <li><code>ProjectSettings</code>: Configuration files for project settings.</li> </ul> </li> <li> <p>Requirements</p> <p> </p> </li> <li> <p>Start</p> <ul> <li>Using Git:</li> <li><code>git clone https://github.com/changdama/COSMIC-PULSES.git</code></li> <li>Download ZIP</li> <li>Open Unity Hub, Click <code>Add</code>and navigate to the folder containing this project, and Select the project and click <code>Open</code></li> </ul> </li> </ul>"},{"location":"projects/music_tech/Syncopation/","title":"Exploring the Influence of Syncopation and Loudness on Listener Arousal in Music","text":"<p>Examine the individual and combined effects of syncopation and loudness on listener-reported arousal</p>"},{"location":"projects/music_tech/Syncopation/#introduction","title":"Introduction","text":"<p>Music is a powerful tool for evoking emotions, capable of influencing mood, physiological states, and even behavior. Arousal, one of the primary dimensions of emotional response, reflects the intensity or energy level of an experience and is shaped by various musical features. Understanding the mechanisms behind arousal in music not only deepens our comprehension of human emotional processing but also has practical applications in fields such as music therapy, entertainment, and artificial intelligence-driven music composition. In this research, using syncopation and loudness to analyze arousal.</p> <ul> <li> <p>Syncopation: Syncopation, marked by rhythmic irregularities and disruptions of expected patterns, evokes excitement and emotional tension by challenging the listener's predictive models (Huron &amp; Ommen, 2006;                     Tan et al., 2019). Its impact on arousal depends on context and familiarity, with moderate syncopation often bringing more enjoyment than very low or high levels, supporting optimal complexity                     theories (G\u00f3mez et al., 2005).</p> </li> <li> <p>Loudness\uff1a Loudness, measured by RMS levels, strongly influences arousal, with louder music perceived as more intense and engaging, triggering heightened physiological and emotional responses (Hosken et al.,                   2021). Unlike syncopation, it serves as a direct cue, consistently correlating with arousal across styles and listeners.</p> </li> </ul>"},{"location":"projects/music_tech/Syncopation/#research-question-and-hypothesis","title":"Research Question and Hypothesis","text":""},{"location":"projects/music_tech/Syncopation/#question","title":"Question","text":"<ul> <li>Does higher syncopation evoke greater arousal in listeners?</li> <li>Does increased loudness correlate with higher arousal levels?</li> </ul>"},{"location":"projects/music_tech/Syncopation/#hypothesis","title":"Hypothesis","text":"<ul> <li>More syncopation within vocal parts evokes higher levels of arousal in listeners.</li> <li>Higher RMS levels (representing loudness) lead to increased levels of arousal.</li> </ul>"},{"location":"projects/music_tech/Syncopation/#method","title":"Method","text":""},{"location":"projects/music_tech/Syncopation/#datasets","title":"Datasets","text":"<ul> <li> <p>Filter <code>*.varms.hum</code>: </p> </li> </ul>"},{"location":"projects/music_tech/Syncopation/#installation","title":"Installation","text":"<ul> <li>Prerequsites</li> </ul> <ul> <li>HumdrumR Installation <pre><code>install.packages('devtools')\ndevtools::install_github(\"Computational-Cognitive-Musicology-Lab/humdrumR\")\ngit clone https://github.com/Computational-Cognitive-Musicology-Lab/humdrumR\ndevtools::install()\nlibrary(humdrumR)\n</code></pre></li> </ul>"},{"location":"projects/music_tech/Syncopation/#key-features","title":"Key Features","text":"<p>\ud83c\udfb6 Syncopation Score   - A measure of rhythmic complexity  - Calculated as the normalized count of syncopations within vocal parts for each piece</p> <p>\ud83d\udd0a RMS Score  - Represents loudness  - Computed as the average RMS value extracted from the <code>**rms</code> spine of each piece.</p> <p>\ud83d\ude0a\u26a1 Arousal Score   - An overall mean value of each piece derived from listener-reported arousal levels in the <code>**arousal</code> spine.</p> <p>\ud83c\udfb6\ud83d\udd0aCombined RMS and Syncopation Score  - A composite score that integrates both RMS and syncopation metrics.</p>"},{"location":"projects/music_tech/Syncopation/#procedure","title":"Procedure","text":"<ul> <li>Load library <pre><code> library(humdrumR)\n library(ggplot2)\n library(dplyr)\n library(tidyr)\n</code></pre></li> <li>Load Files <pre><code>readHumdrum('./.*hum')-&gt;Cocopops\n</code></pre></li> <li>Calculating Syncopation Scores</li> <li> <p>Analyze the syncopation of each piece. First, obtain the duration of each piece and convert them into numerical values. Group the data according to the      \"Piece\".   <pre><code> dur_values &lt;- Cocopops|&gt;\n     group_by(Piece) |&gt;\n     mutate(Duration = as.numeric(as.character(duration(Token)))) |&gt;\n ungroup()\n</code></pre></p> </li> <li> <p>Using \u201csyncopation(dur, meter = duple(5), levels = \u201call\u201d, groupby = list())\u201d in HumdrumR  to access songs in 4/4 time and obtain syncopations. The result is      true and flase. True represents syncopations, and flase does not.     <pre><code>Synco &lt;- dur_values |&gt;\n  group_by(Piece) |&gt;\n  summarise(Syncopation = list(syncopation(Duration, meter = duple(5), levels = \"all\", groupby = list())))\n</code></pre></p> </li> <li> <p>Calculate the number of \u201ctrue\u201d in each piece, that is, the number of syncopations, and use \u201cgeom_col\u201d in \u201cggplot\u201d to plot a bar chart about piece and      Syncopation_Count.     <pre><code>results &lt;- Synco |&gt; \n           mutate(Syncopation_Count = sapply(Syncopation, function(x) sum(x)))\nggplot(results, aes(x = as.factor(Piece), y = Syncopation_Count)) +\n           geom_col() +\n           labs(title = \"Syncopation Count per Piece\", x = \"Piece\", y = \"Syncopation Count\") + theme_minimal()\nggsave(\"plot.jpg\", width = 8, height = 6, dpi = 800)\n</code></pre> </p> </li> <li> <p>Syncopation score for each piece is calculated by taking the Syncopation_Count statistics for all 100 pieces, arranging all the data from smallest to largest, and normalizing to [0,100] to obtain the      corresponding score, and make all Syncopation_Score as scatter plots.</p> <p><pre><code>min_valueS &lt;- min(results$Syncopation_Count)\nmax_valueS &lt;- max(results$Syncopation_Count)\nS_Score_list &lt;- results |&gt; \nmutate(Syncopation_Score = (results$Syncopation_Count - min_valueS) / (max_valueS - min_valueS) * 100)\n\nprint(S_Score_list$Syncopation_Score)\nggplot(S_Score_list, aes(x = Piece, y = Syncopation_Score)) +\n    geom_point(color = \"black\", size = 3) +\n    labs(\n      x = \"Piece\", \n      y = \"Syncopation Score\", \n      title = \"Syncopation Score by Piece\"\n    )\nggsave(\"Syncopation Score.png\", width = 8, height = 6, dpi = 800)\n</code></pre> </p> </li> <li> <p>Calculating RMS Scores</p> </li> <li>Design a function (RMS)that calculates the average of the total rms value of all rows in each song.First, the data in the **rms spine of the corpus is filtered, and the remaining spine is removed, grouping       each piece. Second,Convert humdrum format to data.frame. Load all rms values into col1 and ensure that all data is numeric. Calculate the average of all values in col1, which is the average rms value of the       piece.      <pre><code>RMS&lt;- function(data) {\n      data |&gt;\n      filter(Exclusive == \"rms\") |&gt;\n      group_by(Piece, Bar) |&gt;\n      removeEmptySpines() |&gt;\n      as.data.frame() |&gt;\n      separate(V1, into = \"Col1\", sep = \" \") |&gt;\n      mutate(Col1 = as.numeric(Col1)) |&gt;\n      summarise(mean_col1 = mean(Col1, na.rm = TRUE)) |&gt;\n      ungroup()\n}\n</code></pre></li> <li>Iterate 100 pieces and calculate the rms mean value of all the songs.(Because using \u201cas.data.frame\u201d to calculate the mean causes the \u201cgroup_by(Piece,Bar)\u201d function in HumdrumR to be broken, if the input is       the entire Cocopops corpus, the result is the unique mean of all pieces and all phrases.) Extract the rms average of all pieces and write them to a list, convert to a data frame, and finally visualize them.      <pre><code>RMS_list&lt;- list()\nfor (i in 1:100) {\n   RMS_result &lt;- RMS(Cocopops[i])\n   RMS_list[[i]] &lt;- RMS_result$mean_col1\n }\n mean_col1_list &lt;- lapply(RMS_list, function(x) as.numeric(x))\n\n # Convert to data frame\n mean_col1_df &lt;- data.frame(mean_col1 = unlist(mean_col1_list))\n\n # View results and visualization\n print(mean_col1_df )\n\n ggplot(mean_col1_df, aes(x = seq_along(mean_col1), y = mean_col1)) +\n geom_point(size = 2, color = \"black\") \nlabs(\n   x = \"Piece\", \n   y = \"RMSValue\", \n   title = \"RMSValue by Piece\"\n  ) +\n  theme_minimal()\nggsave(\"RMSValue.png\", width = 8, height = 6, dpi = 800)\n</code></pre></li> <li> <p>For subsequent correlation analysis, we also normalized the average rms value of all pieces from small to large to 0-100 as the rms score of each song and visualize them.      <pre><code>min_value_means_col1 &lt;- min(mean_col1_df$mean_col1)\nmax_value_means_col1 &lt;- max(mean_col1_df$mean_col1)\nRms_Score = (mean_col1_df$mean_col1- min_value_means_col1) / (max_value_means_col1 - min_value_means_col1) * 100\nRms_Score_df &lt;- data.frame(Piece = seq_along(Rms_Score), Rms_Score = Rms_Score)\nggplot(Rms_Score_df, aes(x = Piece, y = Rms_Score)) +\n    geom_point(size = 3, color = \"black\") +  \n    labs(\n      x = \"Piece\", \n      y = \"RMS Score\", \n      title = \"RMS Score Plot\"\n    ) +\n    theme_minimal()\nggsave(\"RMS Score.png\", width = 8, height = 6, dpi = 800)\n</code></pre> </p> </li> <li> <p>Calculating Arousal Score of each piece</p> </li> <li>Design a function (AROUSAL)that calculates the average of the total arousal value of each piece. Since the arousal values come from four people, but in the corpus the four values are in one column, the average score for each phrase cannot be calculated. Therefore,      the **.hum file must first be converted into a data frame, and then the data of one column  from four people into four columns of data from one person, so that the average of the four columns of data can be calculated as the arousal value of each musical phrase      of a piece. Admittedly, participants did not give scores to some musical phrases, so all \u201cNone\u201d were changed to 0.Finally, the arousal value of each musical phrase was summed and then averaged to be used as the arousal value of the entire piece.     <pre><code>AROUSAL &lt;- function(data) {\n  average_result &lt;- data |&gt;\n  filter(Exclusive == \"arousal\") |&gt;\n  group_by(Piece, Bar) |&gt;\n  removeEmptySpines() |&gt;\n  as.data.frame() |&gt;\n  separate(V1, into = c(\"Col1\", \"Col2\", \"Col3\", \"Col4\"), sep = \" \") |&gt;\n  mutate(across(starts_with(\"Col\"), ~ replace_na(as.numeric(.), 0))) |&gt;  # Replace NA with 0\n  rowwise() |&gt;\n  mutate(row_mean = mean(c_across(starts_with(\"Col\")), na.rm = TRUE)) |&gt; # Calculate the average of the rows\n  ungroup()\n\noverall_mean &lt;- average_result |&gt;\n  summarise(overall_row_mean = mean(row_mean, na.rm = TRUE))  # Calculate the overall average\n\nreturn(list(average_result = average_result, overall_mean = overall_mean))\n}\n</code></pre></li> <li>Iterate 100 pieces and calculate the arousal mean value of all the pieces.(Because using \u201cas.data.frame\u201d to calculate the mean causes the \u201cgroup_by(Piece,Bar)\u201d function in HumdrumR to be broken, if the input is the entire Cocopops corpus, the result is the unique      mean of all pieces and all phrases.)     <pre><code>AROUSAL_list &lt;- list()\nfor (i in 1:100) {\n  AROUSAL_result &lt;- AROUSAL(Cocopops[i])\n  AROUSAL_list[[i]] &lt;- AROUSAL_result$overall_mean\n}\n</code></pre></li> <li>Extract the average arousal value(overall_row_mean) of all pieces and write them to a list, convert to a data frame, and finally visualize them.     <pre><code>overall_mean_list &lt;- lapply(AROUSAL_list, function(x) as.numeric(x$overall_row_mean))\noverall_mean_df &lt;- data.frame(overall_mean = unlist(overall_mean_list))\n\nprint(overall_mean_df)\nggplot(overall_mean_df, aes(x = seq_along(overall_mean), y = overall_mean)) +\n  geom_point(color = \"blue\", size = 3) +\n  labs(x = \"Index\", y = \"Overall Mean\", title = \"Scatter Plot of Arousal Value\") +\n  theme_minimal()\n</code></pre></li> <li>For subsequent correlation analysis, we also normalized the average arousal value(overall mean) of all pieces from small to large to 0-100 as the arousal score of each song and visualize them.       <pre><code>min_valueA &lt;- min(overall_mean_df$overall_mean)\nmax_valueA &lt;- max(overall_mean_df$overall_mean)\nA_Score_list &lt;- overall_mean_df |&gt; \n   mutate(Arousal_Score = (overall_mean_df$overall_mean - min_valueA) / (max_valueA - min_valueA) * 100)\n\n print(A_Score_list$Arousal_Score)\n\nggplot(A_Score_list, aes(x = seq_along(Arousal_Score), y = Arousal_Score)) +\n     geom_point(size = 3, color = \"black\") +\n     labs(\n          x = \"Piece\", \n          y = \"Arousal Score\", \n          title = \"Arousal Score Per Piece\"\n         ) +\n          theme_minimal()\nggsave(\"Arousal Score.png\", width = 8, height = 6, dpi = 800)\n</code></pre> </li> </ul>"},{"location":"projects/music_tech/Syncopation/#results","title":"Results","text":""},{"location":"projects/music_tech/Syncopation/#the-correlation-between-syncopation-score-and-arousal-score","title":"The correlation between syncopation score and arousal score","text":"<p>We employed a multiple linear regression model to examine the relationship between the Syncopation score and the Arousal score. The analysis revealed a  negative    relationship (coefficient = -0.1626, p = 0.1711), with minimal variance explained (R\u00b2 = 0.01903), which suggests that as the syncopation score increases, the arousal score tends    to decrease, contrary to hypothesis.</p>"},{"location":"projects/music_tech/Syncopation/#the-correlation-between-rms-score-and-arousal-score","title":"The correlation between RMS score and arousal score","text":"<p>RMS scores positively correlated with arousal (coefficient = 0.7341, p = 0.000719). The positive slope in the multiple linear regression model (Fig. 6) indicates that as RMS score     increases, arousal score also increases, supporting our hypothesis. Although the coefficient between RMS Score and arousal score is greater than 0, the R\u00b2 value is only 0.1107, which     means that the relationship between the two is not convincing.</p>"},{"location":"projects/music_tech/Syncopation/#combined-analysis","title":"Combined Analysis","text":"<p>While examining the relationships between our variables, we created a new combined variable integrating the Syncopation and RMS scores to assess its correlation with the Arousal Score.    Based on the syncopation coefficient of -0.1487 and the RMS coefficient of -0.7242, the overall coefficient is still greater than 0, and the relationship between the independent variable    and the dependent variable is still positively correlated. Compared to the effect of the RMS score on the arousal score alone, the R-squared increases to 0.1266, which means that after    combining the syncopation score, the variable factors that affect the arousal score are more convincing.</p>"},{"location":"projects/music_tech/Syncopation/#comparative-analysis","title":"Comparative Analysis","text":"<p> To explore the relationships between syncopation, RMS values, and arousal levels, we conducted a comparative analysis focusing on selected pieces from the corpus. Specifically, we examined two pieces with extreme syncopation scores: one with the highest syncopation score of 100 (Commodores \u2013 Nightshift) and another with the lowest score of 0 (Jimmy Reed - Baby What You Want Me To Do). For additional context, we also analyzed other pieces with notable syncopation and RMS scores. </p>"},{"location":"projects/music_tech/Syncopation/#syncopation-analysis","title":"Syncopation Analysis","text":"<p>The height of the box plot for each measure represents the proportion of the syncopation count to the total number of notes in the entire measure. When the box plot appears, the higher the height, the smaller the proportion of the syncopation count in the measure. Conversely, the lower the height, the higher the proportion of the syncopation counts in the measure. When the piece does not contain syncopation, no box plot will be displayed, and only the number of non-syncopation notes will be displayed. In Nightshift, syncopation exhibits significant variability, with a range spanning 30 to 100 events per measure. This variability is reflected in the dynamic changes in rhythm, as evidenced by the fluctuating box sizes in the visualization. The larger the number of syncopations of the piece, the greater the variability of different box heights, indicating frequent shifts in rhythmic structure. Conversely, Exploring the Influence of Syncopation and Loudness on Listener Arousal in Music syncopation function in HumdrumR has not detected any syncopated events in the vocal part of Baby What You Want Me To Do, as shown by the absence of boxes in its visualization.</p>"},{"location":"projects/music_tech/Syncopation/#rms-analysis","title":"RMS Analysis","text":"<p>The RMS values per note reveal that in Nightshift, RMS fluctuates between 60 and 80 with considerable instability, while in Baby What You Want Me To Do, RMS values remain relatively stable, fluctuating between 50 and 60.Although higher syncopation often coincides with concentrated RMS values, there is no direct dependence between the magnitude of RMS and the number of syncopations, as RMS consistently remains within the 50\u201375 range across all analyzed pieces.</p>"},{"location":"projects/music_tech/Syncopation/#arousal-analysis","title":"Arousal Analysis","text":"<p>In Nightshift, the high number of syncopations correlates with significant fluctuations in participant-reported arousal levels, indicating a varied emotional response. In contrast, Baby What You Want Me To Do exhibits stable arousal values with minimal fluctuation, consistent with its lack of syncopation.</p>"},{"location":"projects/music_tech/Syncopation/#other-pieces-analysis","title":"Other Pieces Analysis","text":"<p>As for other pieces we select, explaining the relationship between the arousal value and syncopation count, certain pieces (Piece 52 and Piece 56) show a positive relation-ship between syncopation and arousal fluctuations, others (Piece 100 and Piece 83) do not. This inconsistency may explain the weak overall correlation observed between syncopation and arousal scores across the corpus. For explaining the relationship be-tween RMS value and arousal value, no definitive relationship emerged between RMS values and arousal at the individual note level. This suggests the need to consider additional factors, such as harmonic complexity or lyrical content, to holistically understand arousal.</p>"},{"location":"projects/music_tech/Syncopation/#limitation-and-implication","title":"Limitation and Implication","text":"<p>This analysis focused exclusively on vocal parts, excluding instrumental syncopation, which may have contributed additional rhythmic complexity. Furthermore, the dataset was limited to 100 pieces, potentially restricting the generalizability of the findings to broader musical contexts.</p> <p>The results highlight the importance of considering multiple interacting variables in understanding musical arousal. While RMS appears to dominate as a predictor, syncopation may serve as an auxiliary feature that interacts with other musical elements to produce nuanced emotional effects. Future research could explore these interactions using larger and more diverse datasets, as well as experimental designs that manipulate syncopation and loudness simultaneously. From an application perspective, understanding the combined effects of RMS and syncopation could inform the design of more emotionally engaging music in therapeutic, entertainment, or AI-driven music composition contexts. For instance, adaptive music systems could dynamically adjust both loudness and rhythmic complexity to evoke desired emotional responses in listeners.</p>"},{"location":"projects/music_tech/Syncopation/#team","title":"Team","text":"Name Seniority Major Department Contribution Changda Ma Masters Architecture ARCH musical symbolical analysis and visualization Alexey Voronin Masters Music technology MUSI music theory concept"},{"location":"projects/music_tech/music_hide/","title":"Music Hide","text":"<p>AR-based multiplayer musical treasure hunt</p>"},{"location":"projects/music_tech/music_hide/#description","title":"Description","text":"<ul> <li>We built an AR-based multiplayer musical treasure hunt. We used Apple\u2019s AR frameworks - RealityKit and ARKit for development and our target devices were iPhones and iPads.</li> <li>This game was developed to be a \u2018Participant\u2019 DGD1 category, but it\u2019s interesting that we noticed people playing it as a \u2018Wanderer\u2019 where even though they found all the chords, they\u2019d still go around to explore the environment.</li> </ul>"},{"location":"projects/music_tech/music_hide/#game-flow","title":"Game Flow","text":"<p>The game has two roles for players - hider and seeker. The player that opens the game and starts the session becomes the hider by default and the player that joins the session is the seeker. - The hider can move around the physical space with their device and place 4 chords (C/F/G/Amin) in various locations, preferably far apart from each other. The chords are always placed on a flat surface utilizing the plane detection capabilities of the framework to increase augmented realism. - The seeker can join the session by bringing their device close to the hider\u2019s device. The game shows interactive instructions to make sure the two devices are connected and have successfully been able to synchronize their AR worlds. This is done using ARKit along with the Multipeer Connectivity framework for continuous wireless transmission between the devices. - The objective of the seeker is to find all the chords using auditory cues. The 4 chords act as invisible spatialized sound sources in the physical space i.e. chords that are nearby will sound louder than ones that are far away. - Once the seeker feels they are close to a chord, they can tap the screen and the game makes the chord visible if they were close enough to it, otherwise the game tells you that there are no chords around.</p>"},{"location":"projects/music_tech/music_hide/#challenges-and-changes","title":"Challenges and Changes","text":"<ul> <li>Fixing errors related to wireless synchronization of AR maps between devices was tricky and had bugs. (Some are still there)</li> <li>Setting up the parameters and choosing appropriate samples for spatial audio sound so that users could distinguish sounds was challenging. Overall, I think this worked fairly well since we saw users being able to isolate sound sources during the neo-arcade event.</li> </ul>"},{"location":"projects/music_tech/music_hide/#collaboration","title":"Collaboration","text":"<ul> <li>Dhruv: Worked on the technical design and programming of the game</li> <li>Changda: Game theme ideation, how to involve musical ideas, 3D modeling, assist programming</li> </ul>"},{"location":"projects/music_tech/music_hide/#video","title":"Video","text":""},{"location":"projects/music_tech/music_hide/#future-plan","title":"Future Plan","text":""},{"location":"projects/music_tech/musicgen_rhythm/","title":"MusicGen-Rhythm","text":"<p>Rhythm-Aware Conditioning for Text-to-Music Generation</p> <p>MusicGen-Rhythm explores rhythm-aware conditioning strategies for text-to-music generation by extending MusicGen-Small with audio-derived rhythmic representations. The project investigates whether explicit rhythmic cues extracted from audio can improve temporal coherence and rhythmic stability beyond text-only prompting, while keeping the MusicGen backbone frozen.</p>"},{"location":"projects/music_tech/musicgen_rhythm/#motivation","title":"Motivation","text":"<p>Text prompts alone often under-specify rhythm and long-term temporal structure. This work studies how beat- and energy-aware features can guide text-to-music generation without relying on symbolic representations or retraining large generative models.</p>"},{"location":"projects/music_tech/musicgen_rhythm/#method-overview","title":"Method Overview","text":"<p>The pipeline consists of:</p> <ol> <li>Energy-aware audio clipping  </li> <li>Multi-channel rhythm feature extraction  </li> <li>Lightweight rhythm encoder  </li> <li>Fusion with text embeddings  </li> <li>Frozen MusicGen decoder for RVQ token generation  </li> <li>Auxiliary beat- and energy-aware loss design for rhythmic consistency supervision</li> </ol> <p></p>"},{"location":"projects/music_tech/musicgen_rhythm/#rhythm-representation","title":"Rhythm Representation","text":"<p>We construct a four-channel rhythm-focused feature map capturing complementary temporal cues:</p> <ul> <li>Log-Mel spectrogram  </li> <li>\u0394 Log-Mel spectrogram  </li> <li>Spectral flux  </li> <li>RMS energy  </li> </ul> <p></p>"},{"location":"projects/music_tech/musicgen_rhythm/#conditioning-strategies","title":"Conditioning Strategies","text":"<p>Two lightweight fusion mechanisms are explored:</p> <p>Gating-based Fusion </p> <p>Cross-Attention Fusion </p> <p>These designs allow flexible integration of rhythmic information while preserving the original MusicGen generation process.</p>"},{"location":"projects/music_tech/musicgen_rhythm/#loss-design","title":"Loss Design","text":"<p>To encourage rhythmic coherence while keeping the pretrained MusicGen backbone frozen, we introduce auxiliary beat- and energy-aware supervision applied to lightweight prediction heads.</p> <p>Specifically, intermediate decoder representations are used to predict: - a beat activation trajectory \\( \\hat{b}(t) \\), and - an energy envelope \\( \\hat{e}(t) \\),</p> <p>which are aligned with corresponding targets \\( b(t) \\) and \\( e(t) \\) extracted from reference audio.</p>"},{"location":"projects/music_tech/musicgen_rhythm/#beat-and-energy-correlation-losses","title":"Beat and Energy Correlation Losses","text":"<p>We define beat and energy losses using normalized correlation-based objectives, encouraging temporal alignment rather than exact magnitude matching:</p> \\[ \\mathcal{L}_{\\text{beat}} = 1 - \\frac{\\sum_t (\\hat{b}(t) - \\bar{\\hat{b}})(b(t) - \\bar{b})} {\\sqrt{\\sum_t (\\hat{b}(t) - \\bar{\\hat{b}})^2}  \\sqrt{\\sum_t (b(t) - \\bar{b})^2}} \\] \\[ \\mathcal{L}_{\\text{energy}} = 1 - \\frac{\\sum_t (\\hat{e}(t) - \\bar{\\hat{e}})(e(t) - \\bar{e})} {\\sqrt{\\sum_t (\\hat{e}(t) - \\bar{\\hat{e}})^2}  \\sqrt{\\sum_t (e(t) - \\bar{e})^2}} \\] <p>These objectives focus on temporal structure and rhythmic alignment, rather than absolute value regression, making them robust to scale differences across musical excerpts.</p>"},{"location":"projects/music_tech/musicgen_rhythm/#total-training-objective","title":"Total Training Objective","text":"<p>The auxiliary losses are combined with the original MusicGen cross-entropy loss:</p> \\[ \\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{CE}} + \\lambda_{\\text{beat}} \\mathcal{L}_{\\text{beat}} + \\lambda_{\\text{energy}} \\mathcal{L}_{\\text{energy}} \\] <p>where \\( \\lambda_{\\text{beat}} \\) and \\( \\lambda_{\\text{energy}} \\) control the relative influence of rhythmic supervision.</p>"},{"location":"projects/music_tech/musicgen_rhythm/#qualitative-illustration","title":"Qualitative Illustration","text":"<p>The figure below shows example trajectories of predicted and target beat activations and energy envelopes over token time, illustrating how auxiliary supervision encourages alignment in rhythmic structure.</p> <p></p> <p>This loss design enables rhythm-aware learning while preserving the stability and generative capacity of the frozen MusicGen decoder.</p>"},{"location":"projects/music_tech/musicgen_rhythm/#experimental-setup-summary","title":"Experimental Setup (Summary)","text":"<ul> <li>Backbone: facebook/musicgen-small (frozen)  </li> <li>Audio: 32 kHz, 30-second clips  </li> <li>Training &amp; evaluation: Google Colab A100 High RAM (GPU)  </li> </ul> <p>Quantitative evaluations and ablation studies are reported in the accompanying paper under preparation. (Detailed results will be added to this page after publication.)</p>"},{"location":"projects/music_tech/musicgen_rhythm/#future-directions","title":"Future Directions","text":"<p>Several extensions are planned for future work, including: </p> <ul> <li>Interactive rhythm-aware prompting and control  </li> <li>Subjective listening studies and perceptual evaluation  </li> </ul> <p>These directions aim to further explore how explicit rhythmic structure can support controllable and expressive music generation.</p>"},{"location":"projects/music_tech/musicgen_rhythm/#current-status","title":"Current Status","text":"<ul> <li>Core architecture and training pipeline completed  </li> <li>Objective evaluation finalized  </li> <li>Paper submission in preparation  </li> </ul> <p>This page presents methodological design only. Results and conclusions may change after published.</p>"},{"location":"projects/spatial_analytics/","title":"Overview","text":""},{"location":"projects/spatial_analytics/#projects","title":"Projects","text":""},{"location":"projects/spatial_analytics/#gis","title":"GIS:","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> XR Instruments     Multi-headset collaborative XR musical instruments and OSC workflows. Team: Changda Ma, ... \u2192 Learn more</p> </li> </ul>"},{"location":"projects/spatial_analytics/#spatial-regression","title":"Spatial Regression:","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> XR Instruments     Multi-headset collaborative XR musical instruments and OSC workflows. Team: Changda Ma, ... \u2192 Learn more</p> </li> </ul>"},{"location":"projects/spatial_analytics/#geoai","title":"GeoAI:","text":"<ul> <li> <p> Noise &amp; Sleep Disturbance     Urban noise exposure, sleep disturbance index, and health analytics. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> MusicGen-Rhythm     Rhythm-conditioned text-to-music generation with coherence losses. Team: Changda Ma, ... \u2192 Learn more</p> </li> <li> <p> XR Instruments     Multi-headset collaborative XR musical instruments and OSC workflows. Team: Changda Ma, ... \u2192 Learn more</p> </li> </ul>"}]}